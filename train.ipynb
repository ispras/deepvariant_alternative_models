{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef387df7-1924-487c-8cb4-8c0e40efa9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from clu import metric_writers\n",
    "from clu import periodic_actions\n",
    "import ml_collections\n",
    "from ml_collections.config_flags import config_flags\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_providers\n",
    "from deepvariant import dv_utils\n",
    "import keras_modeling\n",
    "from official.modeling import optimization\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "_LEADER = flags.DEFINE_string(\n",
    "    'leader',\n",
    "    'local',\n",
    "    (\n",
    "        'The leader flag specifies the host-controller. Possible values: '\n",
    "        '(1) local=runs locally. If GPUs are available they will be detected'\n",
    "        ' and used.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "_STRATEGY = flags.DEFINE_enum(\n",
    "    'strategy',\n",
    "    'mirrored',\n",
    "    ['tpu', 'mirrored'],\n",
    "    'The strategy to use.',\n",
    ")\n",
    "\n",
    "_EXPERIMENT_DIR = flags.DEFINE_string(\n",
    "    'experiment_dir',\n",
    "    '',\n",
    "    (\n",
    "        'The directory where the model weights, training/tuning summaries, '\n",
    "        'and backup information are stored.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "_LIMIT = flags.DEFINE_integer(\n",
    "    'limit', None, 'Limit the number of steps used for train/eval.'\n",
    ")\n",
    "\n",
    "_DEBUG = flags.DEFINE_bool(\n",
    "    'debug', False, 'Run tensorflow eagerly in debug mode.'\n",
    ")\n",
    "\n",
    "config_flags.DEFINE_config_file('config', \"experiment/configs/nasnetlarge-random-h2.py:base\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9a0007-3fd2-4416-9c69-1e1c5e51b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggerWriter:\n",
    "    def __init__(self, log_func):\n",
    "        self.log_func = log_func\n",
    "    \n",
    "    def write(self, message):\n",
    "        if message.strip():\n",
    "            self.log_func(message.strip())\n",
    "    \n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "def start_logging(log_file):\n",
    "\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    logging.set_verbosity(logging.DEBUG)\n",
    "    \n",
    "    logging.get_absl_handler().use_absl_log_file(ml_collections.ConfigDict(FLAGS.config).log_file)   \n",
    "    \n",
    "    sys.stdout = LoggerWriter(logging.info)  # print() -> INFO\n",
    "    sys.stderr = LoggerWriter(logging.error)  # -> ERROR\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9b0de1-b474-40f4-953e-54d8eb62c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: ml_collections.ConfigDict):\n",
    "  \"\"\"Train a model.\"\"\"\n",
    "  logging.info('Running with debug=%s', _DEBUG.value)\n",
    "  tf.config.run_functions_eagerly(_DEBUG.value)\n",
    "  if _DEBUG.value:\n",
    "    tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "  start_logging(config.log_file)\n",
    "  print(config)\n",
    "  experiment_dir = config.experiment_dir #_EXPERIMENT_DIR.value\n",
    "\n",
    "  model_dir = f'{experiment_dir}/checkpoints'\n",
    "  logging.info(\n",
    "      'Use TPU at %s', _LEADER.value if _LEADER.value is not None else 'local'\n",
    "  )\n",
    "  logging.info('experiment_dir: %s', experiment_dir)\n",
    "  if _STRATEGY.value == 'tpu':\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "        tpu=_LEADER.value\n",
    "    )\n",
    "    tf.config.experimental_connect_to_cluster(resolver, protocol='grpc+loas')\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "  elif _STRATEGY.value in ['mirrored']:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "  else:\n",
    "    raise ValueError(f'Unknown strategy: {_STRATEGY.value}')\n",
    "\n",
    "  # Load config\n",
    "  train_dataset_config = data_providers.read_dataset_config(\n",
    "      config.train_dataset_pbtxt\n",
    "  )\n",
    "\n",
    "  tune_dataset_config = data_providers.read_dataset_config(\n",
    "      config.tune_dataset_pbtxt\n",
    "  )\n",
    "\n",
    "  input_shape = dv_utils.get_shape_from_examples_path(\n",
    "      train_dataset_config.tfrecord_path\n",
    "  )\n",
    "\n",
    "  # Copy example_info.json to checkpoint path.\n",
    "  example_info_json_path = os.path.join(\n",
    "      os.path.dirname(train_dataset_config.tfrecord_path), 'example_info.json'\n",
    "  )\n",
    "  if not tf.io.gfile.exists(example_info_json_path):\n",
    "    raise FileNotFoundError(example_info_json_path)\n",
    "  tf.io.gfile.makedirs(experiment_dir)\n",
    "  tf.io.gfile.copy(\n",
    "      example_info_json_path,\n",
    "      os.path.join(experiment_dir, 'example_info.json'),\n",
    "      overwrite=True,\n",
    "  )\n",
    "\n",
    "  steps_per_epoch = train_dataset_config.num_examples // config.batch_size\n",
    "  steps_per_tune = (\n",
    "      config.num_validation_examples\n",
    "      or tune_dataset_config.num_examples // config.batch_size\n",
    "  )\n",
    "\n",
    "  if _LIMIT.value:\n",
    "    steps_per_epoch = _LIMIT.value\n",
    "    steps_per_tune = _LIMIT.value\n",
    "\n",
    "  # =========== #\n",
    "  # Setup Model #\n",
    "  # =========== #\n",
    "\n",
    "  with strategy.scope():\n",
    "    print('---CHECKPOINT: ', config.init_checkpoint, ' ---')\n",
    "    model = keras_modeling.inceptionv3(\n",
    "         input_shape=input_shape,\n",
    "         weights=config.init_checkpoint,    #upd (None)\n",
    "         init_backbone_with_imagenet=config.init_backbone_with_imagenet,    #upd (True)\n",
    "         config=config,\n",
    "    )\n",
    "\n",
    "    # Define Loss Function.\n",
    "    # TODO: Add function for retrieving custom loss fn.\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=config.label_smoothing,\n",
    "        reduction=tf.keras.losses.Reduction.NONE,\n",
    "    )\n",
    "\n",
    "    def compute_loss(probabilities, labels):\n",
    "      per_example_loss = loss_function(y_pred=probabilities, y_true=labels)\n",
    "      # We divide per-replica losses by global batch size and sum this value\n",
    "      # across all replicas to compute average loss scaled by global batch size.\n",
    "      return tf.nn.compute_average_loss(\n",
    "          per_example_loss, global_batch_size=config.batch_size\n",
    "      )\n",
    "\n",
    "    decay_steps = int(\n",
    "        steps_per_epoch * config.learning_rate_num_epochs_per_decay\n",
    "    )\n",
    "\n",
    "    # TODO: Define learning rate via config.\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=config.learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=config.learning_rate_decay_rate,\n",
    "        staircase=True,\n",
    "    )\n",
    "\n",
    "    logging.info(\n",
    "        'Exponential Decay:'\n",
    "        ' initial_learning_rate=%s\\n'\n",
    "        ' decay_steps=%s\\n'\n",
    "        ' learning_rate_decay_rate=%s',\n",
    "        config.learning_rate,\n",
    "        decay_steps,\n",
    "        config.learning_rate_decay_rate,\n",
    "    )\n",
    "\n",
    "    if config.warmup_steps > 0:\n",
    "      warmup_learning_rate = config.learning_rate / 10\n",
    "      logging.info(\n",
    "          'Use LinearWarmup: \\n warmup_steps=%s\\n warmup_learning_rate=%s',\n",
    "          config.warmup_steps,\n",
    "          warmup_learning_rate,\n",
    "      )\n",
    "      learning_rate = optimization.LinearWarmup(\n",
    "          # This is initial learning rate.\n",
    "          warmup_learning_rate=warmup_learning_rate,\n",
    "          after_warmup_lr_sched=learning_rate,\n",
    "          warmup_steps=config.warmup_steps,\n",
    "      )\n",
    "\n",
    "    # Define Optimizer.\n",
    "    # TODO: Add function for retrieving custom optimizer.\n",
    "    if config.optimizer == 'nadam':\n",
    "      optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif config.optimizer == 'rmsprop':\n",
    "      optimizer = tf.keras.optimizers.RMSprop(\n",
    "          learning_rate=learning_rate,\n",
    "          rho=config.rho,\n",
    "          momentum=config.momentum,\n",
    "          epsilon=config.epsilon,\n",
    "      )\n",
    "    else:\n",
    "      raise ValueError(f'Unknown optimizer: {config.optimizer}')\n",
    "\n",
    "  # ================= #\n",
    "  # Setup Checkpoint  #\n",
    "  # ================= #\n",
    "\n",
    "  # The state object allows checkpointing of the model and associated variables\n",
    "  # for the optimizer, step, and train/tune metrics.\n",
    "  ckpt_manager = keras_modeling.create_state(\n",
    "      config,\n",
    "      model_dir,\n",
    "      model,\n",
    "      optimizer,\n",
    "      strategy,\n",
    "  )\n",
    "  print(ckpt_manager)\n",
    "  state = ckpt_manager.checkpoint\n",
    "\n",
    "  @tf.function\n",
    "  def run_train_step(inputs):\n",
    "    model_inputs, labels = inputs\n",
    "\n",
    "    # model_inputs = aug(model_inputs, config)\n",
    "      \n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(model_inputs, training=True)\n",
    "      probabilities = tf.nn.softmax(logits)\n",
    "      train_loss = compute_loss(probabilities=probabilities, labels=labels)\n",
    "\n",
    "    gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    for metric in state.train_metrics[:-1]:\n",
    "      metric.update_state(\n",
    "          y_pred=probabilities,\n",
    "          y_true=labels,\n",
    "      )\n",
    "    state.train_metrics[-1].update_state(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "  @tf.function\n",
    "  def run_tune_step(tune_inputs):\n",
    "    \"\"\"Single non-distributed tune step.\"\"\"\n",
    "    model_inputs, labels = tune_inputs\n",
    "    logits = model(model_inputs, training=False)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    tune_loss = compute_loss(probabilities=probabilities, labels=labels)\n",
    "\n",
    "    for metric in state.tune_metrics[:-1]:\n",
    "      metric.update_state(\n",
    "          y_pred=probabilities,\n",
    "          y_true=labels,\n",
    "      )\n",
    "      state.tune_metrics[-1].update_state(tune_loss)\n",
    "    return tune_loss\n",
    "\n",
    "  @tf.function\n",
    "  def distributed_train_step(iterator):\n",
    "    per_replica_losses = strategy.run(run_train_step, args=(next(iterator),))\n",
    "    state.global_step.assign_add(1)\n",
    "    return strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None\n",
    "    )\n",
    "\n",
    "  @tf.function\n",
    "  def distributed_tune_step(iterator):\n",
    "    per_replica_losses = strategy.run(run_tune_step, args=(next(iterator),))\n",
    "    return strategy.reduce(\n",
    "        tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None\n",
    "    )\n",
    "\n",
    "  # ============== #\n",
    "  # Setup Datasets #\n",
    "  # ============== #\n",
    "  train_ds = data_providers.input_fn(\n",
    "      train_dataset_config.tfrecord_path,\n",
    "      mode='train',\n",
    "      strategy=strategy,\n",
    "      n_epochs=config.num_epochs,\n",
    "      config=config,\n",
    "      limit=_LIMIT.value,\n",
    "  )\n",
    "  tune_ds = data_providers.input_fn(\n",
    "      tune_dataset_config.tfrecord_path,\n",
    "      mode='tune',\n",
    "      strategy=strategy,\n",
    "      config=config,\n",
    "      limit=steps_per_tune,\n",
    "  )\n",
    "\n",
    "  train_iter, tune_iter = iter(train_ds), iter(tune_ds)\n",
    "  num_train_steps = steps_per_epoch * config.num_epochs\n",
    "\n",
    "  logging.info(\n",
    "      (\n",
    "          '\\n\\n'\n",
    "          'Training Examples: %s\\n'\n",
    "          'Batch Size: %s\\n'\n",
    "          'Epochs: %s\\n'\n",
    "          'Steps per epoch: %s\\n'\n",
    "          'Steps per tune: %s\\n'\n",
    "          'Num train steps: %s\\n'\n",
    "          '\\n'\n",
    "      ),\n",
    "      train_dataset_config.num_examples,\n",
    "      config.batch_size,\n",
    "      config.num_epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_tune,\n",
    "      num_train_steps,\n",
    "  )\n",
    "\n",
    "  # ============= #\n",
    "  # Training Loop #\n",
    "  # ============= #\n",
    "\n",
    "  metric_writer = metric_writers.create_default_writer(logdir=experiment_dir)\n",
    "  report_progress = periodic_actions.ReportProgress(\n",
    "      num_train_steps=num_train_steps,\n",
    "      writer=metric_writer,\n",
    "      every_secs=300,\n",
    "      on_steps=[0, num_train_steps - 1],\n",
    "  )\n",
    "\n",
    "  with strategy.scope():\n",
    "\n",
    "    def get_checkpoint_metric():\n",
    "      \"\"\"Returns the metric we are optimizing for.\"\"\"\n",
    "      best_checkpoint_metric_idx = [\n",
    "          f'tune/{x.name}' for x in state.tune_metrics\n",
    "      ].index(config.best_checkpoint_metric)\n",
    "      return state.tune_metrics[best_checkpoint_metric_idx].result().numpy()\n",
    "\n",
    "    best_checkpoint_metric_value = get_checkpoint_metric()\n",
    "\n",
    "    with metric_writers.ensure_flushes(metric_writer):\n",
    "\n",
    "      def run_tune(train_step, epoch, steps_per_tune):\n",
    "        logging.info('Running tune at step=%d epoch=%d', train_step, epoch)\n",
    "        for loop_tune_step in range(steps_per_tune):\n",
    "          tune_step = loop_tune_step + (epoch * steps_per_tune)\n",
    "          with tf.profiler.experimental.Trace('tune', step_num=tune_step, _r=1):\n",
    "            if loop_tune_step % config.log_every_steps == 0:\n",
    "              logging.info(\n",
    "                  'Tune step %s / %s (%s%%)',\n",
    "                  loop_tune_step,\n",
    "                  steps_per_tune,\n",
    "                  round(float(loop_tune_step) / float(steps_per_tune), 1)\n",
    "                  * 100.0,\n",
    "              )\n",
    "            distributed_tune_step(tune_iter)\n",
    "\n",
    "        metric_writer.write_scalars(\n",
    "            train_step,\n",
    "            {f'tune/{x.name}': x.result() for x in state.tune_metrics},\n",
    "        )\n",
    "\n",
    "      for train_step in range(state.global_step.numpy(), num_train_steps):\n",
    "        # Calculate current epoch\n",
    "        epoch = train_step // steps_per_epoch\n",
    "        if train_step % steps_per_epoch == 0:\n",
    "          logging.info('Starting epoch %s', epoch)\n",
    "\n",
    "        # If we are warmstarting, establish an initial best_checkpoint_metric\n",
    "        # value before beginning any training.\n",
    "        if train_step == 0 and (\n",
    "            config.init_checkpoint or config.init_backbone_with_imagenet\n",
    "        ):\n",
    "          logging.info('Performing initial evaluation of warmstart model.')\n",
    "          run_tune(train_step, epoch, steps_per_tune)\n",
    "          best_checkpoint_metric_value = get_checkpoint_metric()\n",
    "          logging.info(\n",
    "              'Warmstart checkpoint best checkpoint metric: %s=%s',\n",
    "              config.best_checkpoint_metric,\n",
    "              best_checkpoint_metric_value,\n",
    "          )\n",
    "          # Reset tune metrics\n",
    "          for metric in state.tune_metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "        # ===== #\n",
    "        # train #\n",
    "        # ===== #\n",
    "        # Calculate full train step.\n",
    "        is_last_step = train_step == (num_train_steps - 1)\n",
    "\n",
    "        with tf.profiler.experimental.Trace('train', step_num=train_step, _r=1):\n",
    "          distributed_train_step(train_iter)\n",
    "\n",
    "          # Quick indication that training is happening.\n",
    "          logging.log_first_n(\n",
    "              logging.INFO, 'Finished training step %d.', 5, train_step\n",
    "          )\n",
    "\n",
    "        # Log metrics\n",
    "        report_progress(train_step)\n",
    "\n",
    "        if (train_step % config.log_every_steps == 0) or is_last_step:\n",
    "          metrics_to_write = {\n",
    "              f'train/{x.name}': x.result() for x in state.train_metrics\n",
    "          }\n",
    "          if isinstance(\n",
    "              optimizer.learning_rate, tf.distribute.DistributedValues\n",
    "          ):\n",
    "            current_learning_rate = optimizer.learning_rate.numpy()\n",
    "          else:\n",
    "            current_learning_rate = optimizer.learning_rate(train_step)\n",
    "          metrics_to_write['train/learning_rate'] = current_learning_rate\n",
    "          metrics_to_write['epoch'] = epoch\n",
    "          metric_writer.write_scalars(\n",
    "              train_step,\n",
    "              metrics_to_write,\n",
    "          )\n",
    "          # Reset train metrics.\n",
    "          for metric in state.train_metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "        # ==== #\n",
    "        # tune #\n",
    "        # ==== #\n",
    "        # Run tune at every epoch, periodically, and at final step.\n",
    "        if (\n",
    "            (train_step > 0 and train_step % steps_per_epoch == 0)\n",
    "            or (train_step > 0 and train_step % config.tune_every_steps == 0)\n",
    "            or is_last_step\n",
    "        ):\n",
    "          run_tune(train_step, epoch, steps_per_tune)\n",
    "\n",
    "          if get_checkpoint_metric() > best_checkpoint_metric_value:\n",
    "            best_checkpoint_metric_value = get_checkpoint_metric()\n",
    "            checkpoint_path = ckpt_manager.save(train_step)\n",
    "            # Reset early stopping counter\n",
    "            state.early_stopping.assign(0)\n",
    "            logging.info(\n",
    "                'Saved checkpoint %s=%s step=%s epoch=%s path=%s',\n",
    "                config.best_checkpoint_metric,\n",
    "                get_checkpoint_metric(),\n",
    "                train_step,\n",
    "                epoch,\n",
    "                checkpoint_path,\n",
    "            )\n",
    "          else:\n",
    "            if (\n",
    "                config.early_stopping_patience\n",
    "                and state.early_stopping.value()\n",
    "                >= config.early_stopping_patience\n",
    "            ):\n",
    "              break\n",
    "            logging.info(\n",
    "                'Skipping checkpoint with %s=%s < previous best %s=%s',\n",
    "                config.best_checkpoint_metric,\n",
    "                get_checkpoint_metric(),\n",
    "                config.best_checkpoint_metric,\n",
    "                best_checkpoint_metric_value,\n",
    "            )\n",
    "            state.early_stopping.assign_add(1)\n",
    "          if config.early_stopping_patience:\n",
    "            metric_writer.write_scalars(\n",
    "                train_step,\n",
    "                {'tune/early_stopping': state.early_stopping.value()},\n",
    "            )\n",
    "\n",
    "          # Reset tune metrics\n",
    "          for metric in state.tune_metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    # After training completes, load the latest checkpoint and create\n",
    "    # a saved model (.pb) and keras model formats.\n",
    "    checkpoint_path = ckpt_manager.latest_checkpoint\n",
    "    print('ckpt_path: ',checkpoint_path)\n",
    "    \n",
    "    if not checkpoint_path:\n",
    "      logging.info('No checkpoint found.')\n",
    "      return\n",
    "\n",
    "    # The latest checkpoint will be the best performing checkpoint.\n",
    "    logging.info('Loading best checkpoint: %s', checkpoint_path)\n",
    "    tf.train.Checkpoint(model).restore(checkpoint_path).expect_partial()\n",
    "\n",
    "    logging.info('Saving model using saved_model format.')\n",
    "    saved_model_dir = checkpoint_path\n",
    "    model.save(saved_model_dir, save_format='tf')\n",
    "    # Copy example_info.json to saved_model directory.\n",
    "    tf.io.gfile.copy(\n",
    "        example_info_json_path,\n",
    "        os.path.join(\n",
    "            saved_model_dir,\n",
    "            'example_info.json',\n",
    "        ),\n",
    "        overwrite=True,\n",
    "    )\n",
    "    print('---finish_good---')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f782a62-1884-4e38-b8df-d4ebc608b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "  keep_running = True\n",
    "  while keep_running:\n",
    "    try:\n",
    "      train(FLAGS.config)\n",
    "      print('---end_train---')\n",
    "      keep_running = False  # Training completed successfully.\n",
    "    except tf.errors.UnavailableError as error:\n",
    "      logging.warning(\n",
    "          'UnavailableError encountered during training: %s.', error\n",
    "      )\n",
    "      print('!exept!')\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac7a9f-4cb6-44fe-9a53-1290a9481ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = sys.argv[:1]\n",
    "logging.set_verbosity(logging.INFO)\n",
    "warnings.filterwarnings(\n",
    "     'ignore', module='tensorflow_addons.optimizers.average_wrapper')\n",
    "try:\n",
    "    app.run(main)\n",
    "except SystemExit:\n",
    "    print('!sys exit!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
